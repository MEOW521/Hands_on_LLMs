{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98b7c00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers: 4.55.4\n",
      "sentencepiece: 0.2.1\n",
      "sentence_transformers: 5.1.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "## 当前代码需要用到的包\n",
    "pkgs = ['transformers', 'sentencepiece', 'sentence_transformers']\n",
    "\n",
    "for pkg in pkgs:\n",
    "    print(f\"{pkg}:\", version(pkg))\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['HF_ENDPOINT'] = \"https://hf-mirror.com\"\n",
    "os.environ['TRANSFORMERS_CACHE'] = \"/root/autodl-tmp/LLMs/.cache/huggingface\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dea006",
   "metadata": {},
   "source": [
    "# Tokenization and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4155fc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5767259dcd814827bd081860c6559d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-4-mini-instruct\",\n",
    "    device_map='cuda',\n",
    "    torch_dtype='auto',\n",
    "    trust_remote_code=False,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/Phi-4-mini-instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32173a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input_ids: tensor([[ 10930,    448,   3719,  39950,   6396,    316,  32145,    395,    290,\n",
      "          62374,  66241,  80785,    403,     13, 115474,   1495,    480,   3034,\n",
      "            419,     13, 200019]], device='cuda:0')\n",
      "Output_ids: tensor([[ 10930,    448,   3719,  39950,   6396,    316,  32145,    395,    290,\n",
      "          62374,  66241,  80785,    403,     13, 115474,   1495,    480,   3034,\n",
      "            419,     13, 200019,  18174,     25,    336,   2768,    512,   6537,\n",
      "          10384,    395,    290, 193145, 147276,    403,    279,  36210,  32145,\n",
      "           4464,     40,   5498,    495,   3719]], device='cuda:0')\n",
      "Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happend.<|assistant|>Subject: Sincere Apologies for the Gardening Mishap\n",
      "\n",
      "Dear Sarah,\n",
      "\n",
      "I hope this email\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happend.<|assistant|>\"\n",
    "\n",
    "# tokenizer the prompt\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to('cuda')\n",
    "\n",
    "# generate the output\n",
    "generation_output = model.generate(\n",
    "    inputs=input_ids,\n",
    "    max_new_tokens=20,\n",
    ")\n",
    "\n",
    "print(f'Input_ids: {input_ids}')\n",
    "print(f'Output_ids: {generation_output}')\n",
    "\n",
    "print(tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dad33dc",
   "metadata": {},
   "source": [
    "# Token Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a9df4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output: torch.Size([1, 4, 768])\n",
      "[CLS]\n",
      "Hello\n",
      "world\n",
      "[SEP]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Model\n",
    "model = AutoModel.from_pretrained(\"microsoft/deberta-v3-small\")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-small\")\n",
    "\n",
    "# tokenization\n",
    "tokens = tokenizer(\"Hello world\", return_tensors='pt')\n",
    "\n",
    "# process the tokens\n",
    "output = model(**tokens)[0]\n",
    "\n",
    "# shape of output\n",
    "print(f'Shape of output: {output.shape}')\n",
    "\n",
    "# 4 tokens include [CLS] and [SEP]\n",
    "for token in tokens['input_ids'][0]:\n",
    "    print(tokenizer.decode(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3efeae3",
   "metadata": {},
   "source": [
    "# Text Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab263db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62ff4783706b4276926b41f630d62443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13904755f9394542ab78155d3f266cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cb173c041814a5db8149f3038527c74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "200e48484ef64273aada60de74daaf29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cebef1b23124026aa0048d7fbd7b6fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a0dbe47014c4c46b06d5cfa28274cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a71249754d2148b9acaa05b59e69534c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdcd2469a40148dab15bdf184d54a66f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b2bae76ef94fddaccda1f3ed434432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0757258a32548449630502d0a3ed6ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf8c4dfe21a434197fa2154fe8fa60d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector.shape: (768,)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# load model\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# convert text to text embeddings\n",
    "vector = model.encode(\"Best movie ever!\")\n",
    "\n",
    "print(f'vector.shape: {vector.shape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
